{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This just parses fastq files, extracts barcodes and makes a file with barcode counts\n",
    "\n",
    "The number of rows in the output csv should be the number of unique barcodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T19:58:45.754385Z",
     "start_time": "2021-06-08T19:58:43.756250Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "\n",
    "import numpy as np\n",
    "import itertools\n",
    "import math\n",
    "import pandas as pd\n",
    "import time\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from ast import literal_eval\n",
    "import csv\n",
    "import venn\n",
    "from numpy import cov\n",
    "from scipy.stats import spearmanr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T19:58:45.803311Z",
     "start_time": "2021-06-08T19:58:45.756872Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# This information has to be input manually\n",
    "# File names, barcode flank sequences\n",
    "\n",
    "SSFlank = 'GGTGGTGACC'\n",
    "\n",
    "refBCs = [['ACAGATAATGACTGT', 'WT'], \n",
    "          ['GCAATCAAAGATCTG', 'WT'], \n",
    "          ['CCCTTTGGACGGCTG', 'WT'], \n",
    "          ['GACATTATGTTCAAA', 'K191A'], \n",
    "          ['GACCGCGGATACCAA', 'K191A'], \n",
    "          ['TAGCACTTCCCGCAC', 'K191A']]\n",
    "\n",
    "experimentName = 'NP_11_27_selection_first'\n",
    "\n",
    "np112723Data = '/Volumes/NP_DFS_4TB/NPDFS01/NP-11-27-23_S3_L001_R1_001.fastq'\n",
    "np112724Data = '/Volumes/NP_DFS_4TB/NPDFS01/NP-11-27-24_S4_L001_R1_001.fastq'\n",
    "np112725Data = '/Volumes/NP_DFS_4TB/NPDFS01/NP-11-27-25_S5_L001_R1_001.fastq'\n",
    "\n",
    "listOfFiles = [np112723Data, np112724Data, np112725Data]\n",
    "listOfFileNames = ['NP_11_27_23', 'NP_11_27_24', 'NP_11_27_25']\n",
    "listOfLibTypes = ['SS', 'SS', 'SS']\n",
    "\n",
    "MINIMUM_READS = 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T19:58:46.048978Z",
     "start_time": "2021-06-08T19:58:45.806906Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NP_11_27_23\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Volumes/NP_DFS_4TB/NPDFS01/NP-11-27-23_S3_L001_R1_001.fastq'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-433fc1cd8082>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlistOfFiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlistOfFileNames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mdfBClist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparseAndExtractBC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlistOfLibTypes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlistOfFileNames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mdfBClist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Volumes/NP_DFS_4TB/NPDFS01/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistOfFileNames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_BClist.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-433fc1cd8082>\u001b[0m in \u001b[0;36mparseAndExtractBC\u001b[0;34m(file, libtype)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlibtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DS'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mBCFlank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDSFlank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mparsedFile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeqIO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fastq\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mBClist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/Bio/SeqIO/__init__.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(handle, format, alphabet)\u001b[0m\n\u001b[1;32m    605\u001b[0m     \u001b[0miterator_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FormatToIterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0miterator_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mAlignIO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FormatToIterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m         \u001b[0;31m# Use Bio.AlignIO to read in the alignments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/Bio/SeqIO/QualityIO.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, source, alphabet, title2ids)\u001b[0m\n\u001b[1;32m   1057\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The alphabet argument is no longer supported\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle2ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtitle2ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1059\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Fastq\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/Bio/SeqIO/Interfaces.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, source, alphabet, mode, fmt)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The alphabet argument is no longer supported\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_close_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# not a path, assume we received a stream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Volumes/NP_DFS_4TB/NPDFS01/NP-11-27-23_S3_L001_R1_001.fastq'"
     ]
    }
   ],
   "source": [
    "# Parse and extract barcode, make csv files for each one\n",
    "\n",
    "def parseAndExtractBC(file, libtype):\n",
    "    BCFlank = ''\n",
    "    if libtype == 'SS':\n",
    "        BCFlank = SSFlank\n",
    "    if libtype == 'SD':\n",
    "        BCFlank = SDFlank\n",
    "    if libtype == 'DS':\n",
    "        BCFlank = DSFlank\n",
    "    parsedFile = SeqIO.parse(file, \"fastq\")\n",
    "    BClist = []\n",
    "    \n",
    "    for readCount, rec in enumerate(parsedFile):\n",
    "        if readCount % 10000000 == 0:\n",
    "            print(readCount)\n",
    "        readSeq = str(rec.seq)\n",
    "        BCloc = readSeq.find(BCFlank)\n",
    "        if BCloc == -1:\n",
    "            BClist.append('No flank found')\n",
    "        else:\n",
    "            BC = readSeq[BCloc + len(BCFlank):BCloc + len(BCFlank) + 15]\n",
    "            if 'N' in BC:\n",
    "                BClist.append('BC has Ns')\n",
    "            else:\n",
    "                BClist.append(BC)\n",
    "    return BClist\n",
    "\n",
    "# These files are big, save them on a drive\n",
    "for i, file in enumerate(listOfFiles):\n",
    "    print(listOfFileNames[i])\n",
    "    dfBClist = pd.DataFrame(parseAndExtractBC(file, listOfLibTypes[i]), columns = [listOfFileNames[i]])\n",
    "    dfBClist.to_csv('/Volumes/NP_DFS_4TB/NPDFS01/' + listOfFileNames[i] + '_BClist.csv')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T19:58:46.051641Z",
     "start_time": "2021-06-08T19:58:44.635Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Collapse down to smaller list with reads per barcode measured\n",
    "\n",
    "for i, file in enumerate(listOfFiles):\n",
    "    dfBCs = pd.read_csv('/Volumes/NP_DFS_4TB/NPDFS01/' + listOfFileNames[i] + '_BClist.csv')\n",
    "    dfBCCounts = dfBCs[listOfFileNames[i]].value_counts()\n",
    "    dfBCCounts.to_csv(listOfFileNames[i] + '_BCCounts.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T20:03:33.778133Z",
     "start_time": "2021-06-08T20:03:07.496352Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is the PacBio data that made it through all of the filters\n",
    "# 'correctedAAmut' might have a different name eventually\n",
    "lookupTable = pd.read_csv('NP_11_21_1_correctedLookupTable.csv', usecols = ('correctedAAmut','Barcode'))\n",
    "\n",
    "# Add in WT and K191A barcodes (specified above)\n",
    "dfRefs = pd.DataFrame(refBCs, columns = lookupTable.columns)\n",
    "lookupTable = lookupTable.append(dfRefs)\n",
    "\n",
    "combinedDF = lookupTable\n",
    "\n",
    "for i, experiment in enumerate(listOfFileNames):\n",
    "    dfData = pd.read_csv(listOfFileNames[i] + '_BCCounts.csv', index_col = 0)\n",
    "    dfData.index.name = 'Barcode'\n",
    "    combinedDF = combinedDF.merge(dfData, on = 'Barcode', how = 'outer')\n",
    "combinedDF = combinedDF.convert_dtypes()\n",
    "\n",
    "# This is the remaining pacbiodata\n",
    "# Need to collapse it to uniques first\n",
    "PacBioBCTable = pd.read_csv('PacBio_BCCounts.csv', usecols = ('Barcode_sequence','BCfrequency'))\n",
    "PacBioBCTable.rename(columns = {'BCfrequency' : 'Raw_PacBio_counts', 'Barcode_sequence' : 'Barcode'}, \n",
    "                     inplace = True)\n",
    "PacBioBCTable.drop_duplicates(inplace = True)\n",
    "combinedDF = combinedDF.merge(PacBioBCTable, how = 'outer', on = 'Barcode').convert_dtypes()\n",
    "\n",
    "# Retrieve original AA, mutant AA and the position (corrected)\n",
    "def parseMut(row):\n",
    "    mut = row['correctedAAmut']\n",
    "    if pd.isnull(mut):\n",
    "        return np.nan, np.nan, np.nan\n",
    "    if mut == 'WT':\n",
    "        return 'WT', 'WT', 'WT'\n",
    "    else:\n",
    "        originalAA = mut[:1]\n",
    "        mutAA = str(mut[-1:])\n",
    "        mutPos = int(mut[1:-1])\n",
    "        return originalAA, mutAA, mutPos\n",
    "\n",
    "combinedDF['MutParsed'] = combinedDF.apply(lambda row: parseMut(row), axis=1)\n",
    "combinedDF[['OriginalAA','MutAA', 'AAPosition']] = pd.DataFrame(combinedDF.MutParsed.tolist(), \n",
    "                                                                index= combinedDF.index)\n",
    "combinedDF.drop(columns = 'MutParsed', inplace = True)\n",
    "\n",
    "# Load reference data\n",
    "refData = pd.read_csv('biochemData060821.csv')\n",
    "combinedDF = combinedDF.merge(refData, left_on = 'correctedAAmut', right_on = 'Mutation', how = 'outer')\n",
    "combinedDF = combinedDF.convert_dtypes()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T20:06:36.105280Z",
     "start_time": "2021-06-08T20:03:33.780328Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw enrichment calculated\n",
      "WT norm enrichment calculated\n",
      "Average enrichment calculated\n",
      "Barcode average enrichments calculated\n",
      "Barcodes per mut calculated\n",
      "Stats calculated\n"
     ]
    }
   ],
   "source": [
    "# Find enrichment values, do this in the simplest way possible\n",
    "# Don't use PacBio for this\n",
    "# Exclude anything with fewer than 5 reads before or after selection for now\n",
    "# Normalize to WT\n",
    "\n",
    "def calcEnrich(row, column, refColumn):\n",
    "    enrichment = np.nan\n",
    "    if not (pd.isnull(row[column]) or pd.isnull(row[refColumn])):\n",
    "        if (row[column] >= MINIMUM_READS) and row[refColumn] >= MINIMUM_READS:\n",
    "            enrichment = row[column]/row[refColumn]\n",
    "    return enrichment\n",
    "\n",
    "combinedDF['Rep1RawEnrichment'] = combinedDF.apply(lambda row: calcEnrich(row, 'NP_11_27_24', \n",
    "                                                                          'NP_11_27_23'), axis=1)\n",
    "combinedDF['Rep2RawEnrichment'] = combinedDF.apply(lambda row: calcEnrich(row, 'NP_11_27_25', \n",
    "                                                                          'NP_11_27_23'), axis=1)\n",
    "\n",
    "print('Raw enrichment calculated')\n",
    "\n",
    "# Divide reads a given barcode has by the mean of WT reads (all 3 barcodes)\n",
    "# Further normalization to the total number of reads doesn't seem necessary or helpful\n",
    "\n",
    "WTmeanCounts = combinedDF[combinedDF['correctedAAmut'] == 'WT'][['NP_11_27_23', 'NP_11_27_24', \n",
    "                                                                 'NP_11_27_25']].mean()\n",
    "\n",
    "def calcEnrichWTnorm(row, postSel, preSel, WTinPost):\n",
    "    enrichment = np.nan\n",
    "    if not (pd.isnull(row[postSel]) or pd.isnull(row[preSel])):\n",
    "        if row[postSel] >= MINIMUM_READS and row[preSel] >= MINIMUM_READS:\n",
    "            enrichment = (row[postSel]/WTinPost)/(row[preSel]/WTmeanCounts[0])\n",
    "    return enrichment\n",
    "\n",
    "combinedDF['WT_norm_enrich_rep1'] = combinedDF.apply(lambda row: calcEnrichWTnorm(row, \\\n",
    "                                    'NP_11_27_24', 'NP_11_27_23', WTmeanCounts[1]), axis=1)\n",
    "combinedDF['WT_norm_enrich_rep2'] = combinedDF.apply(lambda row: calcEnrichWTnorm(row, \\\n",
    "                                    'NP_11_27_25', 'NP_11_27_23', WTmeanCounts[2]), axis=1)\n",
    "\n",
    "print('WT norm enrichment calculated')\n",
    "\n",
    "\n",
    "# This will need to be rewritten for more than 2 replicates\n",
    "def averageEnrichCalc(row):\n",
    "    return np.mean([row['WT_norm_enrich_rep1'], row['WT_norm_enrich_rep2']])\n",
    "\n",
    "combinedDF['WT_norm_enrich_avg'] = combinedDF.apply(lambda row: averageEnrichCalc(row), axis=1)\n",
    "\n",
    "print('Average enrichment calculated')\n",
    "\n",
    "\n",
    "# Add columns for means and std. dev.s\n",
    "# The mean is the mean of the average enrichments between replicates across all barcodes of a given mutation\n",
    "\n",
    "means = pd.DataFrame(combinedDF.groupby('correctedAAmut')['WT_norm_enrich_avg'].mean()).rename(columns = \n",
    "                                                            {'WT_norm_enrich_avg' : 'mut_BCs_mean'})\n",
    "combinedDF = combinedDF.merge(means, on = 'correctedAAmut', how = 'outer')\n",
    "stdDevs = pd.DataFrame(combinedDF.groupby('correctedAAmut')['WT_norm_enrich_avg'].std()).rename(columns = \n",
    "                                                            {'WT_norm_enrich_avg' : 'mut_BCs_stdDev'})\n",
    "combinedDF = combinedDF.merge(stdDevs, on = 'correctedAAmut', how = 'outer')\n",
    "\n",
    "print('Barcode average enrichments calculated')\n",
    "\n",
    "\n",
    "# Find how many barcodes there are for each mutant\n",
    "\n",
    "BCsPerMut = pd.DataFrame(combinedDF.groupby('correctedAAmut')['Barcode'].nunique()).rename(columns = \n",
    "                                                            {'Barcode' : 'Number_of_BCs_for_mut'})\n",
    "combinedDF = combinedDF.merge(BCsPerMut, on = 'correctedAAmut', how = 'outer')\n",
    "combinedDF = combinedDF.convert_dtypes()\n",
    "\n",
    "print('Barcodes per mut calculated')\n",
    "\n",
    "\n",
    "# Do some statistics\n",
    "\n",
    "# Calculate the \"standard score\" from the Kosuri GPCR paper\n",
    "# \"Note the standard score here is (x−μ)/σ where x is the forskolin ratio of that barcode\n",
    "# μ is the mean forskolin ratio of the mutant that barcode corresponds to, and σ is the \n",
    "# standard deviation of the forskolin ratio of the mutant\"\n",
    "# Coefficient of variation is just the barcode average divided by the barcode stdDev\n",
    "#  We need to be careful to only calculate stats for barcodes with enough data\n",
    "#  I don't include barcodes with null values for WT_norm_enrich_avg\n",
    "\n",
    "def calcStdScore(row):\n",
    "    stdScore = np.nan\n",
    "    x = row['WT_norm_enrich_avg']\n",
    "    mu = row['mut_BCs_mean']\n",
    "    sig = row['mut_BCs_stdDev']\n",
    "    if sig != 0:\n",
    "        stdScore = (x - mu)/sig\n",
    "    return stdScore\n",
    "\n",
    "def calcCoeffVar(row):\n",
    "    coeffVar = np.nan\n",
    "    mu = row['mut_BCs_mean']\n",
    "    sig = row['mut_BCs_stdDev']\n",
    "    x = row['WT_norm_enrich_avg']\n",
    "    if sig != 0 and not math.isnan(x):\n",
    "        coeffVar = mu/sig\n",
    "    return coeffVar\n",
    "\n",
    "combinedDF['Standard_score'] = combinedDF.apply(lambda row: calcStdScore(row), axis=1)\n",
    "combinedDF['Coefficient_of_variation'] = combinedDF.apply(lambda row: calcCoeffVar(row), axis=1)\n",
    "combinedDF.convert_dtypes()\n",
    "\n",
    "print('Stats calculated')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T20:06:36.125576Z",
     "start_time": "2021-06-08T20:06:36.111811Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Barcode', 'correctedAAmut', 'NP_11_27_23', 'NP_11_27_24',\n",
       "       'NP_11_27_25', 'Raw_PacBio_counts', 'OriginalAA', 'MutAA', 'AAPosition',\n",
       "       'Mutation', 'vcMUT/vcWT', 'Error (fraction of fraction)', 'Sc/o',\n",
       "       'Sc/o error (+/-)', 'citation', 'Rep1RawEnrichment',\n",
       "       'Rep2RawEnrichment', 'WT_norm_enrich_rep1', 'WT_norm_enrich_rep2',\n",
       "       'WT_norm_enrich_avg', 'mut_BCs_mean', 'mut_BCs_stdDev',\n",
       "       'Number_of_BCs_for_mut', 'Standard_score', 'Coefficient_of_variation'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combinedDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T20:06:58.185556Z",
     "start_time": "2021-06-08T20:06:36.128653Z"
    }
   },
   "outputs": [],
   "source": [
    "combinedDF.to_csv(experimentName + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
